{
  "version": "1.0",
  "cells": [
    {
      "type": "markdown",
      "content": "# Classification Metrics\n\nIn the last notebook we used a binary classifier to predict whether patients were diabetic or not. We used *accuracy*, the proportion of how many cases were predicted correctly, as a measure of how well the model performed, but accuracy isn't everything. In this notebook, we will look at alternatives to accuracy that can be much more useful in machine learning.\n\n## Alternative metrics for binary classifiers\n\nAccuracy seems like a sensible metric to evaluate (and to a certain extent it is), but you need to be careful about drawing too many conclusions from the accuracy of a classifier. Suppose only 3% of the population is diabetic. You could create a classifier that just always predicts 0, and it would be 97% accurate - but not terribly helpful in identifying patients with diabetes!\n\nFortunately, there are some other metrics that reveal more about how our model is performing. Scikit-Learn includes the ability to create a *classification report* that provides more insight than raw accuracy alone.\n\n## Load the data\n1. Download [diabetes.csv](https://raw.githubusercontent.com/MicrosoftLearning/mslearn-ml-basics/refs/heads/main/Labfiles/data/diabetes.csv){:target=\"_blank\"} in a new browser tab, and save it on your local disk.\n2. Then use the **Upload Data** button at the top of this notebook to upload it.\n3. Run the next cell by clicking the **&#9658; Run** button to load the data and train a classification model."
    },
    {
      "type": "python",
      "content": "import pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\n# load the training dataset\ndiabetes = pd.read_csv('diabetes.csv')\n\n# Separate features and labels\nfeatures = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\nlabel = 'Diabetic'\nX, y = diabetes[features].values, diabetes[label].values\n\n\n# Split data 70%-30% into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\nprint ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))\n\n# Train the model\nfrom sklearn.linear_model import LogisticRegression\n\n# Set regularization rate\nreg = 0.01\n\n# train a logistic regression model on the training set\nmodel = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nprint('Predicted labels: ', predictions)\nprint('Actual labels:    ', y_test)\n\n\nprint('Accuracy: ', accuracy_score(y_test, predictions))\n"
    },
    {
      "type": "markdown",
      "content": "One of the simplest places to start is a classification report. Run the next cell to see a range of alternate ways to assess our model."
    },
    {
      "type": "python",
      "content": "from sklearn. metrics import classification_report\n\nprint(classification_report(y_test, predictions))"
    },
    {
      "type": "markdown",
      "content": "The classification report includes the following metrics for each class (0 and 1):\n\n* *Precision*: Of the predictions the model made for this class, what proportion were correct?\n* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?\n* *F1-Score*: An average metric that takes both precision and recall into account.\n* *Support*: How many instances of this class are there in the test dataset?\n\nThe classification report also includes averages for these metrics, including a weighted average that allows for the imbalance in the number of cases of each class.\n\nBecause this is a *binary* classification problem, the ***1*** class is considered *positive* and its precision and recall are particularly interesting - these in effect answer the questions:\n\n    - Of all the patients the model predicted are diabetic, how many are actually diabetic?\n    - Of all the patients that are actually diabetic, how many did the model identify?\n    \n\nYou can retrieve these values on their own by using the **precision_score** and **recall_score** metrics in Scikit-Learn (which by default assume a binary classification model)."
    },
    {
      "type": "python",
      "content": "from sklearn.metrics import precision_score, recall_score\n\nprint(\"Overall Precision:\", precision_score(y_test, predictions))\nprint(\"Overall Recall:\", recall_score(y_test, predictions))"
    },
    {
      "type": "markdown",
      "content": "The precision and recall metrics are derived from four possible prediction outcomes:\n* *True Positives*: The predicted label and the actual label are both 1.\n* *False Positives*: The predicted label is 1, but the actual label is 0.\n* *False Negatives*: The predicted label is 0, but the actual label is 1.\n* *True Negatives*: The predicted label and the actual label are both 0.\n\nThese metrics are generally tabulated for the test set and shown together as a *confusion matrix*, which takes the following form:\n\n<table style=\"border: 1px solid black; width: 50px;margin-left: auto; margin-right: auto;\">\n    <tr style=\"border: 1px solid black;\">\n        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP</td>\n    </tr>\n    <tr style=\"border: 1px solid black;\">\n        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP</td>\n    </tr>\n</table>\n\nNote that the correct (*true*) predictions form a diagonal line from top left to bottom right - these figures should be significantly higher than the *false* predictions if the model is any good.\n\nIn Python, you can use the **sklearn.metrics.confusion_matrix** function to find these values for a trained classifier:"
    },
    {
      "type": "python",
      "content": "from sklearn.metrics import confusion_matrix\n\n# Print the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nprint (cm)"
    },
    {
      "type": "markdown",
      "content": "It's common to plot a confusion matrix as a *heat map* with shade of each cell reflecting its value. In an ideal model, the shading forms a diagonal pattern of darker shades from top-left to bottom-right, indicating higher values where the *predicted* and *actual* labels match."
    },
    {
      "type": "python",
      "content": "import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Create the confusion matrix\ncm = confusion_matrix(y_test, predictions)\n\n# Create a figure and axis\nplt.figure(figsize=(8, 6))\n\n# Create a heatmap using matplotlib\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\n# Add labels\nclasses = ['No Diabetes', 'Diabetes']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\n# Add text annotations\nthresh = cm.max() / 2.\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, format(cm[i, j], 'd'),\n                ha=\"center\", va=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.show()"
    },
    {
      "type": "markdown",
      "content": "Until now, we've considered the predictions from the model as being either 1 or 0 class labels. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logistic regression, are based on *probability*. What actually gets predicted by a binary classifier is the probability that the label is true (**P(y)**) and the probability that the label is false (1 - **P(y)**). A threshold value of 0.5 is used to decide whether the predicted label is a 1 (*P(y) > 0.5*) or a 0 (*P(y) <= 0.5*). You can use the **predict_proba** method to see the probability pairs for each case:"
    },
    {
      "type": "python",
      "content": "y_scores = model.predict_proba(X_test)\nprint(y_scores)"
    },
    {
      "type": "markdown",
      "content": "The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilities are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the *true positive rate* (which is another name for recall) and the *false positive rate* for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a *received operator characteristic (ROC) chart*, like this:"
    },
    {
      "type": "python",
      "content": "from sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()"
    },
    {
      "type": "markdown",
      "content": "The ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction - you want the curve to be higher than that (or your model is no better than simply guessing!).\n\nThe area under the curve (*AUC*) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model. Scikit-Learn includes a function to calculate this metric, **roc_auc_score**."
    },
    {
      "type": "python",
      "content": "from sklearn.metrics import roc_auc_score\n\nauc = roc_auc_score(y_test,y_scores[:,1])\nprint('AUC: ' + str(auc))"
    },
    {
      "type": "markdown",
      "content": "### Perform preprocessing in a pipeline\n\nIn this case, the ROC curve and its AUC indicate that the model performs better than a random guess which is not bad considering we performed very little preprocessing of the data.\n\nIn practice, it's common to perform some preprocessing of the data to make it easier for the algorithm to fit a model to it. There's a huge range of preprocessing transformations you can perform to get your data ready for modeling, but we'll limit ourselves to a few common techniques:\n\n- Scaling numeric features so they're on the same scale. This prevents features with large values from producing coefficients that disproportionately affect the predictions.\n- Encoding categorical variables. For example, by using a *one-hot encoding* technique you can create individual binary (true/false) features for each possible category value.\n\nTo apply these preprocessing transformations, we'll make use of a Scikit-Learn feature named *pipelines*. Pipelines enable us to define a set of preprocessing steps that end with an algorithm. You can then apply the entire pipeline to the data, so that the model encapsulates all of the preprocessing steps as well as the regression algorithm. This is useful, because when we want to use the model to predict values from new data, we'll need to apply the same transformations (based on the same statistical distributions and category encodings used with the training data).\n\n> **Note**: The term *pipeline* is used extensively in machine learning, often to mean very different things! In this context, we're using it to refer to pipeline objects in Scikit-Learn."
    },
    {
      "type": "python",
      "content": "# Train the model\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Define preprocessing for numeric columns (normalize them so they're on the same scale)\nnumeric_features = [0,1,2,3,4,5,6]\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\n# Define preprocessing for categorical features (encode the Age column)\ncategorical_features = [7]\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('logregressor', LogisticRegression(C=1/reg, solver=\"liblinear\"))])\n\n\n# fit the pipeline to train a logistic regression model on the training set\nmodel = pipeline.fit(X_train, (y_train))\nprint (model)"
    },
    {
      "type": "markdown",
      "content": "This pipeline encapsulates the preprocessing steps and also model training.\n\nLet's use the model trained by this pipeline to predict labels for our test set, and compare the performance metrics with the basic model we created previously."
    },
    {
      "type": "python",
      "content": "# Get predictions from test data\npredictions = model.predict(X_test)\ny_scores = model.predict_proba(X_test)\n\n# Get evaluation metrics\ncm = confusion_matrix(y_test, predictions)\nprint ('Confusion Matrix:\\n',cm, '\\n')\nprint('Accuracy:', accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\", precision_score(y_test, predictions))\nprint(\"Overall Recall:\", recall_score(y_test, predictions))\nauc = roc_auc_score(y_test,y_scores[:,1])\nprint('AUC: ' + str(auc))\n\n# calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\n"
    },
    {
      "type": "markdown",
      "content": "The results do look a little better, so clearly preprocessing the data has made a difference.\n\n### Try a different algorithm\n\nNow let's try a different algorithm. Previously we used a logistic regression algorithm, which is a *linear* algorithm. There are many kinds of classification algorithms we could try, including:\n\n- **Support Vector Machine algorithms**: Algorithms that define a *hyperplane* that separates classes.\n- **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction.\n- **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n\nThis time, we'll use the same preprocessing steps as before, but we'll train the model using an *ensemble* algorithm named *Random Forest* that combines the outputs of multiple random decision trees. For more details, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)."
    },
    {
      "type": "python",
      "content": "from sklearn.ensemble import RandomForestClassifier\n\n# Create preprocessing and training pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('logregressor', RandomForestClassifier(n_estimators=100))])\n\n# fit the pipeline to train a random forest model on the training set\nmodel = pipeline.fit(X_train, (y_train))\nprint (model)"
    },
    {
      "type": "markdown",
      "content": "Let's look at the performance metrics for the new model."
    },
    {
      "type": "python",
      "content": "predictions = model.predict(X_test)\ny_scores = model.predict_proba(X_test)\ncm = confusion_matrix(y_test, predictions)\nprint ('Confusion Matrix:\\n',cm, '\\n')\nprint('Accuracy:', accuracy_score(y_test, predictions))\nprint(\"Overall Precision:\",precision_score(y_test, predictions))\nprint(\"Overall Recall:\",recall_score(y_test, predictions))\nauc = roc_auc_score(y_test,y_scores[:,1])\nprint('\\nAUC: ' + str(auc))\n\n# calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()"
    },
    {
      "type": "markdown",
      "content": "That looks better!\n\n### Use the Model for Inferencing\nNow that we have a reasonably useful trained model, we can save it for use later to predict labels for new data."
    },
    {
      "type": "python",
      "content": "import joblib\n\n# Save the model as a pickle file\nfilename = './diabetes_model.pkl'\njoblib.dump(model, filename)"
    },
    {
      "type": "markdown",
      "content": "When we have some new observations with unknown labels, we can load our new model and use it to predict label values."
    },
    {
      "type": "python",
      "content": "# Load the model from the file\nmodel = joblib.load(filename)\n\n# predict on a new sample\n# The model accepts an array of feature arrays (so you can predict the classes of multiple patients in a single call)\n# We'll create an array with a single array of features, representing one patient\nX_new = np.array([[2,180,74,24,21,23.9091702,1.488172308,22]])\nprint ('New sample: {}'.format(list(X_new[0])))\n\n# Get a prediction\npred = model.predict(X_new)\n\n# The model returns an array of predictions - one for each set of features submitted\n# In our case, we only submitted one patient, so our prediction is the first one in the resulting array.\nprint('Predicted class is {}'.format(pred[0]))"
    },
    {
      "type": "markdown",
      "content": "## Summary\n\nIn this notebook, we looked at a range of metrics for binary classification and tried a few algorithms beyond logistic regression. We'll move onto more complex classification problems in the next notebook."
    }
  ]
}